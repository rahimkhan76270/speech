{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.475121 Mb\n",
      "torch.Size([2, 120])\n"
     ]
    }
   ],
   "source": [
    "class GlobalLayerNorm(nn.Module):\n",
    "    '''\n",
    "       Calculate Global Layer Normalization\n",
    "       dim: (int or list or torch.Size) \n",
    "            input shape from an expected input of size\n",
    "       eps: a value added to the denominator for numerical stability.\n",
    "       elementwise_affine: a boolean value that when set to True, \n",
    "           this module has learnable per-element affine parameters \n",
    "           initialized to ones (for weights) and zeros (for biases).\n",
    "    '''\n",
    "\n",
    "    def __init__(self, dim, eps=1e-05, elementwise_affine=True):\n",
    "        super(GlobalLayerNorm, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.eps = eps\n",
    "        self.elementwise_affine = elementwise_affine\n",
    "\n",
    "        if self.elementwise_affine:\n",
    "            self.weight = nn.Parameter(torch.ones(self.dim, 1))\n",
    "            self.bias = nn.Parameter(torch.zeros(self.dim, 1))\n",
    "        else:\n",
    "            self.register_parameter('weight', None)\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = N x C x L\n",
    "        # N x 1 x 1\n",
    "        # cln: mean,var N x 1 x L\n",
    "        # gln: mean,var N x 1 x 1\n",
    "        if x.dim() != 3:\n",
    "            raise RuntimeError(\"{} accept 3D tensor as input\".format(\n",
    "                self.__name__))\n",
    "\n",
    "        mean = torch.mean(x, (1, 2), keepdim=True)\n",
    "        var = torch.mean((x-mean)**2, (1, 2), keepdim=True)\n",
    "        # N x C x L\n",
    "        if self.elementwise_affine:\n",
    "            x = self.weight*(x-mean)/torch.sqrt(var+self.eps)+self.bias\n",
    "        else:\n",
    "            x = (x-mean)/torch.sqrt(var+self.eps)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CumulativeLayerNorm(nn.LayerNorm):\n",
    "    '''\n",
    "       Calculate Cumulative Layer Normalization\n",
    "       dim: you want to norm dim\n",
    "       elementwise_affine: learnable per-element affine parameters \n",
    "    '''\n",
    "\n",
    "    def __init__(self, dim, elementwise_affine=True):\n",
    "        super(CumulativeLayerNorm, self).__init__(\n",
    "            dim, elementwise_affine=elementwise_affine)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: N x C x L\n",
    "        # N x L x C\n",
    "        x = torch.transpose(x, 1, 2)\n",
    "        # N x L x C == only channel norm\n",
    "        x = super().forward(x)\n",
    "        # N x C x L\n",
    "        x = torch.transpose(x, 1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "def select_norm(norm, dim):\n",
    "    if norm not in ['gln', 'cln', 'bn']:\n",
    "        if x.dim() != 3:\n",
    "            raise RuntimeError(\"{} accept 3D tensor as input\".format(\n",
    "                self.__name__))\n",
    "\n",
    "    if norm == 'gln':\n",
    "        return GlobalLayerNorm(dim, elementwise_affine=True)\n",
    "    if norm == 'cln':\n",
    "        return CumulativeLayerNorm(dim, elementwise_affine=True)\n",
    "    else:\n",
    "        return nn.BatchNorm1d(dim)\n",
    "\n",
    "\n",
    "class Conv1D(nn.Conv1d):\n",
    "    '''\n",
    "       Applies a 1D convolution over an input signal composed of several input planes.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(Conv1D, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def forward(self, x, squeeze=False):\n",
    "        # x: N x C x L\n",
    "        if x.dim() not in [2, 3]:\n",
    "            raise RuntimeError(\"{} accept 2/3D tensor as input\".format(\n",
    "                self.__name__))\n",
    "        x = super().forward(x if x.dim() == 3 else torch.unsqueeze(x, 1))\n",
    "        if squeeze:\n",
    "            x = torch.squeeze(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ConvTrans1D(nn.ConvTranspose1d):\n",
    "    '''\n",
    "       This module can be seen as the gradient of Conv1d with respect to its input. \n",
    "       It is also known as a fractionally-strided convolution \n",
    "       or a deconvolution (although it is not an actual deconvolution operation).\n",
    "    '''\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(ConvTrans1D, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def forward(self, x, squeeze=False):\n",
    "        \"\"\"\n",
    "        x: N x L or N x C x L\n",
    "        \"\"\"\n",
    "        if x.dim() not in [2, 3]:\n",
    "            raise RuntimeError(\"{} accept 2/3D tensor as input\".format(\n",
    "                self.__name__))\n",
    "        x = super().forward(x if x.dim() == 3 else torch.unsqueeze(x, 1))\n",
    "        if squeeze:\n",
    "            x = torch.squeeze(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Conv1D_Block(nn.Module):\n",
    "    '''\n",
    "       Consider only residual links\n",
    "    '''\n",
    "\n",
    "    def __init__(self, in_channels=256, out_channels=512,\n",
    "                 kernel_size=3, dilation=1, norm='gln', causal=False):\n",
    "        super(Conv1D_Block, self).__init__()\n",
    "        # conv 1 x 1\n",
    "        self.conv1x1 = Conv1D(in_channels, out_channels, 1)\n",
    "        self.PReLU_1 = nn.PReLU()\n",
    "        self.norm_1 = select_norm(norm, out_channels)\n",
    "        # not causal don't need to padding, causal need to pad+1 = kernel_size\n",
    "        self.pad = (dilation * (kernel_size - 1)) // 2 if not causal else (\n",
    "            dilation * (kernel_size - 1))\n",
    "        # depthwise convolution\n",
    "        self.dwconv = Conv1D(out_channels, out_channels, kernel_size,\n",
    "                             groups=out_channels, padding=self.pad, dilation=dilation)\n",
    "        self.PReLU_2 = nn.PReLU()\n",
    "        self.norm_2 = select_norm(norm, out_channels)\n",
    "        self.Sc_conv = nn.Conv1d(out_channels, in_channels, 1, bias=True)\n",
    "        self.causal = causal\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: N x C x L\n",
    "        # N x O_C x L\n",
    "        c = self.conv1x1(x)\n",
    "        # N x O_C x L\n",
    "        c = self.PReLU_1(c)\n",
    "        c = self.norm_1(c)\n",
    "        # causal: N x O_C x (L+pad)\n",
    "        # noncausal: N x O_C x L\n",
    "        c = self.dwconv(c)\n",
    "        # N x O_C x L\n",
    "        if self.causal:\n",
    "            c = c[:, :, :-self.pad]\n",
    "        c = self.Sc_conv(c)\n",
    "        return x+c\n",
    "\n",
    "\n",
    "class ConvTasNet(nn.Module):\n",
    "    '''\n",
    "       ConvTasNet module\n",
    "       N\tNumber of ﬁlters in autoencoder\n",
    "       L\tLength of the ﬁlters (in samples)\n",
    "       B\tNumber of channels in bottleneck and the residual paths 1 x 1-conv blocks\n",
    "       Sc\tNumber of channels in skip-connection paths 1 x 1-conv blocks\n",
    "       H\tNumber of channels in convolutional blocks\n",
    "       P\tKernel size in convolutional blocks\n",
    "       X\tNumber of convolutional blocks in each repeat\n",
    "       R\tNumber of repeats\n",
    "    '''\n",
    "\n",
    "    def __init__(self,\n",
    "                 N=512,\n",
    "                 L=16,\n",
    "                 B=128,\n",
    "                 H=512,\n",
    "                 P=3,\n",
    "                 X=8,\n",
    "                 R=3,\n",
    "                 norm=\"gln\",\n",
    "                 num_spks=2,\n",
    "                 activate=\"relu\",\n",
    "                 causal=False):\n",
    "        super(ConvTasNet, self).__init__()\n",
    "        # n x 1 x T => n x N x T\n",
    "        self.encoder = Conv1D(1, N, L, stride=L // 2, padding=0)\n",
    "        # n x N x T  Layer Normalization of Separation\n",
    "        self.LayerN_S = select_norm('cln', N)\n",
    "        # n x B x T  Conv 1 x 1 of  Separation\n",
    "        self.BottleN_S = Conv1D(N, B, 1)\n",
    "        # Separation block\n",
    "        # n x B x T => n x B x T\n",
    "        self.separation = self._Sequential_repeat(\n",
    "            R, X, in_channels=B, out_channels=H, kernel_size=P, norm=norm, causal=causal)\n",
    "        # n x B x T => n x 2*N x T\n",
    "        self.gen_masks = Conv1D(B, num_spks*N, 1)\n",
    "        # n x N x T => n x 1 x L\n",
    "        self.decoder = ConvTrans1D(N, 1, L, stride=L//2)\n",
    "        # activation function\n",
    "        active_f = {\n",
    "            'relu': nn.ReLU(),\n",
    "            'sigmoid': nn.Sigmoid(),\n",
    "            'softmax': nn.Softmax(dim=0)\n",
    "        }\n",
    "        self.activation_type = activate\n",
    "        self.activation = active_f[activate]\n",
    "        self.num_spks = num_spks\n",
    "\n",
    "    def _Sequential_block(self, num_blocks, **block_kwargs):\n",
    "        '''\n",
    "           Sequential 1-D Conv Block\n",
    "           input:\n",
    "                 num_block: how many blocks in every repeats\n",
    "                 **block_kwargs: parameters of Conv1D_Block\n",
    "        '''\n",
    "        Conv1D_Block_lists = [Conv1D_Block(\n",
    "            **block_kwargs, dilation=(2**i)) for i in range(num_blocks)]\n",
    "\n",
    "        return nn.Sequential(*Conv1D_Block_lists)\n",
    "\n",
    "    def _Sequential_repeat(self, num_repeats, num_blocks, **block_kwargs):\n",
    "        '''\n",
    "           Sequential repeats\n",
    "           input:\n",
    "                 num_repeats: Number of repeats\n",
    "                 num_blocks: Number of block in every repeats\n",
    "                 **block_kwargs: parameters of Conv1D_Block\n",
    "        '''\n",
    "        repeats_lists = [self._Sequential_block(\n",
    "            num_blocks, **block_kwargs) for i in range(num_repeats)]\n",
    "        return nn.Sequential(*repeats_lists)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() >= 3:\n",
    "            raise RuntimeError(\n",
    "                \"{} accept 1/2D tensor as input, but got {:d}\".format(\n",
    "                    self.__name__, x.dim()))\n",
    "        if x.dim() == 1:\n",
    "            x = torch.unsqueeze(x, 0)\n",
    "        # x: n x 1 x L => n x N x T\n",
    "        # print(\"shape x \",x.shape)\n",
    "        w = self.encoder(x)\n",
    "        # print(\"shape w encoder\",w.shape)\n",
    "        # n x N x L => n x B x L\n",
    "        e = self.LayerN_S(w)\n",
    "        # print(\"shape e layer norm\",e.shape)\n",
    "        e = self.BottleN_S(e)\n",
    "        # print(\"shape e bottle neck\",e.shape)\n",
    "        # n x B x L => n x B x L\n",
    "        e = self.separation(e)\n",
    "        # print(\"shape e separation\",e.shape)\n",
    "        # n x B x L => n x num_spk*N x L\n",
    "        m = self.gen_masks(e)\n",
    "        # n x N x L x num_spks\n",
    "        # print(\"shape m gen mask\",m.shape)\n",
    "        m = torch.chunk(m, chunks=self.num_spks, dim=1)\n",
    "        # num_spks x n x N x L\n",
    "        m = self.activation(torch.stack(m, dim=0))\n",
    "        # print(f\"shape m after activation\",m.shape)\n",
    "        d = [w*m[i] for i in range(self.num_spks)]\n",
    "        # print(f\"shape d[i]\",d[0].shape)\n",
    "        # decoder part num_spks x n x L\n",
    "        s = [self.decoder(d[i], squeeze=True) for i in range(self.num_spks)]\n",
    "        # print(f\"shape s[i] layer norm\",s[0].shape)\n",
    "        return s\n",
    "\n",
    "\n",
    "def check_parameters(net):\n",
    "    '''\n",
    "        Returns module parameters. Mb\n",
    "    '''\n",
    "    parameters = sum(param.numel() for param in net.parameters())\n",
    "    return parameters / 10**6\n",
    "\n",
    "\n",
    "def test_convtasnet():\n",
    "    x = torch.randn(2,120)\n",
    "    nnet = ConvTasNet()\n",
    "    s = nnet(x)\n",
    "    print(str(check_parameters(nnet))+' Mb')\n",
    "    print(s[1].shape)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_convtasnet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sequential_block(num_blocks, **block_kwargs):\n",
    "        '''\n",
    "           Sequential 1-D Conv Block\n",
    "           input:\n",
    "                 num_block: how many blocks in every repeats\n",
    "                 **block_kwargs: parameters of Conv1D_Block\n",
    "        '''\n",
    "        Conv1D_Block_lists = [Conv1D_Block(\n",
    "            **block_kwargs, dilation=(2**i)) for i in range(num_blocks)]\n",
    "\n",
    "        return nn.Sequential(*Conv1D_Block_lists)\n",
    "\n",
    "def Sequential_repeat(num_repeats, num_blocks, **block_kwargs):\n",
    "    '''\n",
    "        Sequential repeats\n",
    "        input:\n",
    "                num_repeats: Number of repeats\n",
    "                num_blocks: Number of block in every repeats\n",
    "                **block_kwargs: parameters of Conv1D_Block\n",
    "    '''\n",
    "    repeats_lists = [Sequential_block(\n",
    "        num_blocks, **block_kwargs) for _ in range(num_repeats)]\n",
    "    return nn.Sequential(*repeats_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=ConvTasNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "out=model(torch.rand(size=[1,16_00]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3475121"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(param.numel() for param in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
