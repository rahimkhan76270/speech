{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,L,N):\n",
    "        super(Encoder,self).__init__()\n",
    "        \"\"\"\n",
    "        L: Number of input channels\n",
    "        N: Number of output channels\n",
    "        \"\"\"\n",
    "        self.L = L\n",
    "        self.N = N\n",
    "        self.EPS = 1e-8\n",
    "        self.conv1d_U=nn.Conv1d(in_channels=L,out_channels=N,kernel_size=1,stride=1,bias=False)\n",
    "        self.conv1d_V=nn.Conv1d(in_channels=L,out_channels=N,kernel_size=1,stride=1,bias=False)\n",
    "    \n",
    "    def forward(self,mixture):\n",
    "        \"\"\"\n",
    "        mixture: Tensor of shape (batch_size, channels, timestasmps) or [B,K,L]\n",
    "        \"\"\"\n",
    "        B,K,L=mixture.size()\n",
    "        norm_coef=torch.norm(mixture,p=2,dim=2,keepdim=True) # [B,K,1]\n",
    "        norm_mixture=mixture/(norm_coef+self.EPS) # [B,K,L]\n",
    "        norm_mixture=torch.unsqueeze(norm_mixture.view(-1,L),2) # [B,1,K,L]\n",
    "        conv=F.relu(self.conv1d_U(norm_mixture)) # [B,N,K,L]\n",
    "        gate=F.sigmoid(self.conv1d_V(norm_mixture))\n",
    "        mixture_w=conv*gate\n",
    "        mixture_w=mixture_w.view(B,K,self.N)\n",
    "        return mixture_w,norm_coef\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seperator(nn.Module):\n",
    "    def __init__(self,N,hidden_size,num_layers,bidirectional=False,nspk=2):\n",
    "        super(Seperator,self).__init__()\n",
    "        \"\"\"\n",
    "        N: Number of input channels\n",
    "        hidden_size: Number of hidden units\n",
    "        num_layers: Number of layers\n",
    "        bidirectional: Whether the RNN is bidirectional\n",
    "        nspk: Number of speakers\n",
    "        \"\"\"\n",
    "        self.N = N\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.nspk = nspk\n",
    "        self.layer_norm=nn.LayerNorm(N)\n",
    "        self.LSTM=nn.LSTM(input_size=N,hidden_size=hidden_size,num_layers=num_layers,bidirectional=bidirectional,batch_first=True)\n",
    "        fc_in_dim=hidden_size*2 if bidirectional else hidden_size\n",
    "        self.fc=nn.Linear(fc_in_dim,nspk*N)\n",
    "\n",
    "    def forward(self,mixture_w,mixture_lengths):\n",
    "        \"\"\"\n",
    "        mixture_w: Tensor of shape (batch_size, N, timestasmps) or [B,N,L]\n",
    "        \"\"\"\n",
    "        B,K,N=mixture_w.size()\n",
    "        norm_mixture_w=self.layer_norm(mixture_w)\n",
    "        total_length=norm_mixture_w.size(1)\n",
    "        packed_input=pack_padded_sequence(norm_mixture_w,mixture_lengths,batch_first=True)\n",
    "        packed_output,hidden=self.LSTM(packed_input)\n",
    "        output,_=pad_packed_sequence(packed_output,total_length=total_length,batch_first=True)\n",
    "        score=self.fc(output)\n",
    "        score=score.view(B,K,self.nspk,N)\n",
    "        est_mask=F.softmax(score,dim=2)\n",
    "        return est_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,N,L):\n",
    "        super(Decoder,self).__init__()\n",
    "        \"\"\"\n",
    "        N: Number of input channels\n",
    "        L: Number of output channels\n",
    "        \"\"\"\n",
    "        self.N = N\n",
    "        self.L = L\n",
    "        self.basis_signals=nn.Linear(N,L,bias=False)\n",
    "    \n",
    "    def forward(self,mixture_w,est_mask,norm_coef):\n",
    "        \"\"\"\n",
    "        est_mask: Tensor of shape (batch_size, N, nspk, timestasmps) or [B,N,K,L]\n",
    "        mixture: Tensor of shape (batch_size, channels, timestasmps) or [B,K,L]\n",
    "        norm_coef: Tensor of shape (batch_size, channels, 1) or [B,K,1]\n",
    "        \"\"\"\n",
    "        source_w=torch.unsqueeze(mixture_w,2)*est_mask # [B,N,K,L]\n",
    "        est_source=self.basis_signals(source_w)\n",
    "        norm_coef=torch.unsqueeze(norm_coef,2)\n",
    "        est_source=est_source*norm_coef\n",
    "        est_source=est_source.permute(0,2,1,3).contiguous()\n",
    "        return est_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TasNet(nn.Module):\n",
    "    def __init__(self,L,N,hidden_size,num_layers,bidirectional=False,nspk=2):\n",
    "        super(TasNet,self).__init__()\n",
    "        self.L=L\n",
    "        self.N=N\n",
    "        self.hidden_size=hidden_size\n",
    "        self.num_layers=num_layers\n",
    "        self.bidirectional=bidirectional\n",
    "        self.nspk=nspk\n",
    "        self.encoder=Encoder(L,N)\n",
    "        self.seperator=Seperator(N,hidden_size,num_layers,bidirectional,nspk)\n",
    "        self.decoder=Decoder(N,L)\n",
    "    \n",
    "    def forward(self,mixture,mixture_lengths):\n",
    "        mixture_w,norm_coef=self.encoder(mixture)\n",
    "        est_mask=self.seperator(mixture_w,mixture_lengths)\n",
    "        est_source=self.decoder(mixture_w,est_mask,norm_coef)\n",
    "        return est_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=TasNet(1,250,250,4,bidirectional=True,nspk=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5767750"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.randn(2,1,40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.2086,  2.0988,  0.8783, -1.0730, -2.4978,  0.3917, -0.2256,\n",
      "          -0.3452, -1.1686, -0.8310,  0.9254, -0.8503,  0.1203,  1.1072,\n",
      "          -0.6407,  1.1583, -0.5139, -0.5353,  0.3703,  0.6892,  0.1710,\n",
      "           1.1443,  0.8203,  0.7230,  1.1424,  0.7780, -1.6464,  1.6767,\n",
      "          -1.3182,  0.3406, -0.2319, -1.9739,  0.4833, -0.3824, -1.1773,\n",
      "          -1.5991,  0.7753, -1.2457,  0.2406, -0.7732]],\n",
      "\n",
      "        [[-1.9884,  1.4801, -0.7135,  0.9549, -1.2234, -0.5150,  0.9733,\n",
      "          -0.5323,  0.8743, -1.2915,  0.3379, -0.0094,  0.6453,  0.2503,\n",
      "          -0.6325,  0.0552, -2.0403, -0.6826,  0.2837, -0.6834,  0.0314,\n",
      "           1.4162, -0.6548, -0.8204,  0.7546,  1.0601,  0.3665, -0.7210,\n",
      "          -0.1676, -0.0032,  0.8184,  0.3159,  0.3440, -0.8520,  1.0428,\n",
      "          -0.0203, -0.7260,  0.2557,  0.7778, -0.1656]]])\n",
      "torch.Size([2, 1, 40])\n"
     ]
    }
   ],
   "source": [
    "print(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder Dataflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "L=1 # input signal length\n",
    "N=250 # number of basis signals\n",
    "hidden_size=250 # number of hidden units\n",
    "num_layers=2 # number of layers\n",
    "bidirectional=True # whether the RNN is bidirectional\n",
    "nspk=2 # number of speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1d_v=nn.Conv1d(in_channels=L,out_channels=N,kernel_size=1,stride=1,bias=False)\n",
    "conv1d_u=nn.Conv1d(in_channels=L,out_channels=N,kernel_size=1,stride=1,bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1d_u_out=conv1d_u(x)\n",
    "conv1d_v_out=conv1d_v(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 250, 40]), torch.Size([2, 250, 40]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1d_u_out.shape,conv1d_v_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 1])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_coef=torch.norm(x,p=2,dim=2,keepdim=True)\n",
    "norm_coef.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 40])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_mixture=x/(norm_coef+1e-8)\n",
    "norm_mixture.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([80, 1, 1])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_mixture=torch.unsqueeze(norm_mixture.view(-1,L),2)\n",
    "norm_mixture.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Created on 2018/12/10\n",
    "# Author: Kaituo XU\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "EPS = 1e-8\n",
    "\n",
    "\n",
    "class TasNet(nn.Module):\n",
    "    def __init__(self, L, N, hidden_size, num_layers,\n",
    "                 bidirectional=True, nspk=2):\n",
    "        super(TasNet, self).__init__()\n",
    "        # hyper-parameter\n",
    "        self.L, self.N = L, N\n",
    "        self.hidden_size, self.num_layers = hidden_size, num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.nspk = nspk\n",
    "        # Components\n",
    "        self.encoder = Encoder(L, N)\n",
    "        self.separator = Separator(N, hidden_size, num_layers,\n",
    "                                   bidirectional=bidirectional, nspk=nspk)\n",
    "        self.decoder = Decoder(N, L)\n",
    "\n",
    "    def forward(self, mixture, mixture_lengths):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            mixture: [B, K, L]\n",
    "            mixture_lengths: [B]\n",
    "        Returns:\n",
    "            est_source: [B, nspk, K, L]\n",
    "        \"\"\"\n",
    "        mixture_w, norm_coef = self.encoder(mixture)\n",
    "        est_mask = self.separator(mixture_w, mixture_lengths)\n",
    "        est_source = self.decoder(mixture_w, est_mask, norm_coef)\n",
    "        return est_source\n",
    "\n",
    "    @classmethod\n",
    "    def load_model(cls, path):\n",
    "        # Load to CPU\n",
    "        package = torch.load(path, map_location=lambda storage, loc: storage)\n",
    "        model = cls.load_model_from_package(package)\n",
    "        return model\n",
    "\n",
    "    @classmethod\n",
    "    def load_model_from_package(cls, package):\n",
    "        model = cls(package['L'], package['N'],\n",
    "                    package['hidden_size'], package['num_layers'],\n",
    "                    bidirectional=package['bidirectional'],\n",
    "                    nspk=package['nspk'])\n",
    "        model.load_state_dict(package['state_dict'])\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def serialize(model, optimizer, epoch, tr_loss=None, cv_loss=None):\n",
    "        package = {\n",
    "            # hyper-parameter\n",
    "            'L': model.L,\n",
    "            'N': model.N,\n",
    "            'hidden_size': model.hidden_size,\n",
    "            'num_layers': model.num_layers,\n",
    "            'bidirectional': model.bidirectional,\n",
    "            'nspk': model.nspk,\n",
    "            # state\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optim_dict': optimizer.state_dict(),\n",
    "            'epoch': epoch\n",
    "        }\n",
    "        if tr_loss is not None:\n",
    "            package['tr_loss'] = tr_loss\n",
    "            package['cv_loss'] = cv_loss\n",
    "        return package\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"Estimation of the nonnegative mixture weight by a 1-D gated conv layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, L, N):\n",
    "        super(Encoder, self).__init__()\n",
    "        # hyper-parameter\n",
    "        self.L = L\n",
    "        self.N = N\n",
    "        # Components\n",
    "        # Maybe we can impl 1-D conv by nn.Linear()?\n",
    "        self.conv1d_U = nn.Conv1d(L, N, kernel_size=1, stride=1, bias=False)\n",
    "        self.conv1d_V = nn.Conv1d(L, N, kernel_size=1, stride=1, bias=False)\n",
    "\n",
    "    def forward(self, mixture):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            mixture: [B, K, L]\n",
    "        Returns:\n",
    "            mixture_w: [B, K, N]\n",
    "            norm_coef: [B, K, 1]\n",
    "        \"\"\"\n",
    "        B, K, L = mixture.size()\n",
    "        # L2 Norm along L axis\n",
    "        norm_coef = torch.norm(mixture, p=2, dim=2, keepdim=True)  # B x K x 1\n",
    "        norm_mixture = mixture / (norm_coef + EPS) # B x K x L\n",
    "        # 1-D gated conv\n",
    "        norm_mixture = torch.unsqueeze(norm_mixture.view(-1, L), 2)  # B*K x L x 1\n",
    "        conv = F.relu(self.conv1d_U(norm_mixture))         # B*K x N x 1\n",
    "        gate = torch.sigmoid(self.conv1d_V(norm_mixture))  # B*K x N x 1\n",
    "        mixture_w = conv * gate  # B*K x N x 1\n",
    "        mixture_w = mixture_w.view(B, K, self.N) # B x K x N\n",
    "        return mixture_w, norm_coef\n",
    "\n",
    "\n",
    "class Separator(nn.Module):\n",
    "    \"\"\"Estimation of source masks\n",
    "    TODO: 1. normlization described in paper\n",
    "          2. LSTM with skip connection\n",
    "    \"\"\"\n",
    "    def __init__(self, N, hidden_size, num_layers, bidirectional=True, nspk=2):\n",
    "        super(Separator, self).__init__()\n",
    "        # hyper-parameter\n",
    "        self.N = N\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.nspk = nspk\n",
    "        # Components\n",
    "        self.layer_norm = nn.LayerNorm(N)\n",
    "        self.rnn = nn.LSTM(N, hidden_size, num_layers,\n",
    "                           batch_first=True,\n",
    "                           bidirectional=bidirectional)\n",
    "        fc_in_dim = hidden_size * 2 if bidirectional else hidden_size\n",
    "        self.fc = nn.Linear(fc_in_dim, nspk * N)\n",
    "        ### To impl LSTM with skip connection\n",
    "        # self.rnn = nn.ModuleList()\n",
    "        # self.rnn += [nn.LSTM(N, hidden_size, num_layers=1,\n",
    "        #                      batch_first=True,\n",
    "        #                      bidirectional=bidirectional)]\n",
    "        # for l in range(1, num_layers):\n",
    "        #     self.rnn += [nn.LSTM(hidden_size, hidden_size, num_layers=1,\n",
    "        #                          batch_first=True,\n",
    "        #                          bidirectional=bidirectional)]\n",
    "\n",
    "    def forward(self, mixture_w, mixture_lengths):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            mixture_w: [B, K, N], padded\n",
    "        Returns:\n",
    "            est_mask: [B, K, nspk, N]\n",
    "        \"\"\"\n",
    "        B, K, N = mixture_w.size()\n",
    "        # layer norm\n",
    "        norm_mixture_w = self.layer_norm(mixture_w)\n",
    "        # LSTM\n",
    "        total_length = norm_mixture_w.size(1)  # get the max sequence length\n",
    "        packed_input = pack_padded_sequence(norm_mixture_w, mixture_lengths,\n",
    "                                            batch_first=True)\n",
    "        packed_output, hidden = self.rnn(packed_input)\n",
    "        output, _ = pad_packed_sequence(packed_output,\n",
    "                                        batch_first=True,\n",
    "                                        total_length=total_length)\n",
    "        # fc\n",
    "        score = self.fc(output)  # B x K x nspk*N\n",
    "        score = score.view(B, K, self.nspk, N)\n",
    "        # softmax\n",
    "        est_mask = F.softmax(score, dim=2)\n",
    "        return est_mask\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, N, L):\n",
    "        super(Decoder, self).__init__()\n",
    "        # hyper-parameter\n",
    "        self.N, self.L = N, L\n",
    "        # Components\n",
    "        self.basis_signals = nn.Linear(N, L, bias=False)\n",
    "\n",
    "    def forward(self, mixture_w, est_mask, norm_coef):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            mixture_w: [B, K, N]\n",
    "            est_mask: [B, K, nspk, N]\n",
    "            norm_coef: [B, K, 1]\n",
    "        Returns:\n",
    "            est_source: [B, nspk, K, L]\n",
    "        \"\"\"\n",
    "        # D = W * M\n",
    "        source_w = torch.unsqueeze(mixture_w, 2) * est_mask  # B x K x nspk x N\n",
    "        # S = DB\n",
    "        est_source = self.basis_signals(source_w)  # B x K x nspk x L\n",
    "        # reverse L2 norm\n",
    "        norm_coef = torch.unsqueeze(norm_coef, 2)  # B x K x 1 x1\n",
    "        est_source = est_source * norm_coef  # B x K x nspk x L\n",
    "        est_source = est_source.permute((0, 2, 1, 3)).contiguous() # B x nspk x K x L\n",
    "        return est_source\n",
    "\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     torch.manual_seed(123)\n",
    "#     B, K, L, N, C = 2, 3, 4, 3, 2\n",
    "#     hidden_size, num_layers = 4, 2\n",
    "#     mixture = torch.randint(3, (B, K, L))\n",
    "#     lengths = torch.LongTensor([K for i in range(B)])\n",
    "#     # test Encoder\n",
    "#     encoder = Encoder(L, N)\n",
    "#     encoder.conv1d_U.weight.data = torch.randint(2, encoder.conv1d_U.weight.size())\n",
    "#     encoder.conv1d_V.weight.data = torch.randint(2, encoder.conv1d_V.weight.size())\n",
    "#     mixture_w, norm_coef = encoder(mixture)\n",
    "#     print('mixture', mixture)\n",
    "#     print('U', encoder.conv1d_U.weight)\n",
    "#     print('V', encoder.conv1d_V.weight)\n",
    "#     print('mixture_w', mixture_w)\n",
    "#     print('norm_coef', norm_coef)\n",
    "\n",
    "#     # test Separator\n",
    "#     separator = Separator(N, hidden_size, num_layers)\n",
    "#     est_mask = separator(mixture_w, lengths)\n",
    "#     print('est_mask', est_mask)\n",
    "\n",
    "#     # test Decoder\n",
    "#     decoder = Decoder(N, L)\n",
    "#     est_mask = torch.randint(2, (B, K, C, N))\n",
    "#     est_source = decoder(mixture_w, est_mask, norm_coef)\n",
    "#     print('est_source', est_source)\n",
    "\n",
    "#     # test TasNet\n",
    "#     tasnet = TasNet(L, N, hidden_size, num_layers)\n",
    "#     est_source = tasnet(mixture, lengths)\n",
    "#     print('est_source', est_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 9.6799e-04, -6.8922e-05, -1.2105e-03,  ..., -7.2665e-03,\n",
      "            4.3924e-04, -8.7612e-04],\n",
      "          [-4.0198e-03, -3.1121e-03,  9.8232e-03,  ..., -5.1843e-03,\n",
      "           -1.0728e-03,  9.0640e-03],\n",
      "          [-1.8041e-04,  1.9591e-03,  1.4766e-03,  ..., -6.5888e-03,\n",
      "           -2.5243e-03,  2.4783e-03]],\n",
      "\n",
      "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 9.2842e-04, -2.3611e-04, -1.3510e-03,  ..., -7.1514e-03,\n",
      "            9.0811e-05, -7.8514e-04],\n",
      "          [-3.3517e-03, -2.6084e-03,  9.7110e-03,  ..., -5.2460e-03,\n",
      "           -1.9317e-03,  9.1394e-03],\n",
      "          [-3.8018e-05,  2.0170e-03,  1.2277e-03,  ..., -6.7110e-03,\n",
      "           -3.0796e-03,  2.6998e-03]]]], grad_fn=<CloneBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import librosa\n",
    "\n",
    "# 1. Load the audio file\n",
    "file_path = \"/mnt/d/Programs/Python/PW/projects/asteroid/zip-hindi-2k/cv/19273.wav\"\n",
    "audio, sr = librosa.load(file_path, sr=None)  # Load with original sampling rate\n",
    "\n",
    "# 2. Convert to a PyTorch tensor\n",
    "audio_tensor = torch.tensor(audio)\n",
    "\n",
    "# 3. Segment the audio (Example: 256 samples per segment)\n",
    "#    This depends on your model's expected input length `L`\n",
    "L = 256  # Length of each time step (number of samples per segment)\n",
    "K = audio_tensor.size(0) // L  # Calculate number of time steps\n",
    "\n",
    "# Truncate or pad the audio if necessary to make it fit exactly into K * L\n",
    "audio_tensor = audio_tensor[:K * L].view(1, K, L)  # Reshape to [B, K, L]\n",
    "\n",
    "# 4. Prepare the batch\n",
    "mixture_lengths = torch.tensor([K])  # Length of the input\n",
    "\n",
    "# 5. Pass the tensor to the TasNet model\n",
    "model = TasNet(L=L, N=250, hidden_size=250, num_layers=2, bidirectional=True, nspk=2)\n",
    "output = model(audio_tensor, mixture_lengths)\n",
    "\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speech-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
