{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from omegaconf import DictConfig,OmegaConf\n",
    "from pyannote.core import Annotation\n",
    "from pyannote.metrics.diarization import DiarizationErrorRate\n",
    "import torch\n",
    "from pytorch_lightning import LightningModule\n",
    "from pytorch_lightning.utilities import rank_zero_only\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from nemo.collections.asr.metrics.der import score_labels\n",
    "from nemo.collections.asr.models.clustering_diarizer import (\n",
    "    _MODEL_CONFIG_YAML,\n",
    "    _SPEAKER_MODEL,\n",
    "    _VAD_MODEL,\n",
    "    get_available_model_names,\n",
    ")\n",
    "from nemo.collections.asr.models.msdd_models import ClusterEmbedding,EncDecDiarLabelModel\n",
    "from nemo.collections.asr.models.configs.diarizer_config import NeuralDiarizerInferenceConfig\n",
    "from nemo.collections.asr.models.label_models import EncDecSpeakerLabelModel\n",
    "from nemo.collections.asr.parts.utils.speaker_utils import (\n",
    "    audio_rttm_map,\n",
    "    get_id_tup_dict,\n",
    "    get_uniq_id_list_from_manifest,\n",
    "    labels_to_pyannote_object,\n",
    "    make_rttm_with_overlap,\n",
    "    rttm_to_labels,\n",
    ")\n",
    "from nemo.core.classes.common import PretrainedModelInfo\n",
    "from nemo.utils import logging\n",
    "\n",
    "try:\n",
    "    from torch.cuda.amp import autocast\n",
    "except ImportError:\n",
    "    from contextlib import contextmanager\n",
    "\n",
    "    @contextmanager\n",
    "    def autocast(enabled=None):\n",
    "        yield"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralDiarizer(LightningModule):\n",
    "    def __init__(self, cfg: Union[DictConfig, NeuralDiarizerInferenceConfig]):\n",
    "        super().__init__()\n",
    "        self._cfg = cfg\n",
    "\n",
    "        # Parameter settings for MSDD model\n",
    "        self.use_speaker_model_from_ckpt = cfg.diarizer.msdd_model.parameters.get('use_speaker_model_from_ckpt', True)\n",
    "        self.use_clus_as_main = cfg.diarizer.msdd_model.parameters.get('use_clus_as_main', False)\n",
    "        self.max_overlap_spks = cfg.diarizer.msdd_model.parameters.get('max_overlap_spks', 2)\n",
    "        self.num_spks_per_model = cfg.diarizer.msdd_model.parameters.get('num_spks_per_model', 2)\n",
    "        self.use_adaptive_thres = cfg.diarizer.msdd_model.parameters.get('use_adaptive_thres', True)\n",
    "        self.max_pred_length = cfg.diarizer.msdd_model.parameters.get('max_pred_length', 0)\n",
    "        self.diar_eval_settings = cfg.diarizer.msdd_model.parameters.get(\n",
    "            'diar_eval_settings', [(0.25, True), (0.25, False), (0.0, False)]\n",
    "        )\n",
    "\n",
    "        self._init_msdd_model(cfg)\n",
    "        self.diar_window_length = cfg.diarizer.msdd_model.parameters.diar_window_length\n",
    "        self.msdd_model.cfg = self.transfer_diar_params_to_model_params(self.msdd_model, cfg)\n",
    "\n",
    "        # Initialize clustering and embedding preparation instance (as a diarization encoder).\n",
    "        self.clustering_embedding = ClusterEmbedding(\n",
    "            cfg_diar_infer=cfg, cfg_msdd_model=self.msdd_model.cfg, speaker_model=self._speaker_model\n",
    "        )\n",
    "\n",
    "        # Parameters for creating diarization results from MSDD outputs.\n",
    "        self.clustering_max_spks = self.msdd_model._cfg.max_num_of_spks\n",
    "        self.overlap_infer_spk_limit = cfg.diarizer.msdd_model.parameters.get(\n",
    "            'overlap_infer_spk_limit', self.clustering_max_spks\n",
    "        )\n",
    "\n",
    "    def transfer_diar_params_to_model_params(self, msdd_model, cfg):\n",
    "        msdd_model.cfg.diarizer.out_dir = cfg.diarizer.out_dir\n",
    "        msdd_model.cfg.test_ds.manifest_filepath = cfg.diarizer.manifest_filepath\n",
    "        msdd_model.cfg.test_ds.emb_dir = cfg.diarizer.out_dir\n",
    "        msdd_model.cfg.test_ds.batch_size = cfg.diarizer.msdd_model.parameters.infer_batch_size\n",
    "        msdd_model.cfg.test_ds.seq_eval_mode = cfg.diarizer.msdd_model.parameters.seq_eval_mode\n",
    "        msdd_model._cfg.max_num_of_spks = cfg.diarizer.clustering.parameters.max_num_speakers\n",
    "        return msdd_model.cfg\n",
    "\n",
    "    @rank_zero_only\n",
    "    def save_to(self, save_path: str):\n",
    "        self.clus_diar = self.clustering_embedding.clus_diar_model\n",
    "        _NEURAL_DIAR_MODEL = \"msdd_model.nemo\"\n",
    "\n",
    "        with tempfile.TemporaryDirectory() as tmpdir:\n",
    "            config_yaml = os.path.join(tmpdir, _MODEL_CONFIG_YAML)\n",
    "            spkr_model = os.path.join(tmpdir, _SPEAKER_MODEL)\n",
    "            neural_diar_model = os.path.join(tmpdir, _NEURAL_DIAR_MODEL)\n",
    "\n",
    "            self.clus_diar.to_config_file(path2yaml_file=config_yaml)\n",
    "            if self.clus_diar.has_vad_model:\n",
    "                vad_model = os.path.join(tmpdir, _VAD_MODEL)\n",
    "                self.clus_diar._vad_model.save_to(vad_model)\n",
    "            self.clus_diar._speaker_model.save_to(spkr_model)\n",
    "            self.msdd_model.save_to(neural_diar_model)\n",
    "            self.clus_diar.__make_nemo_file_from_folder(filename=save_path, source_dir=tmpdir)\n",
    "\n",
    "    def extract_standalone_speaker_model(self, prefix: str = 'msdd._speaker_model.') -> EncDecSpeakerLabelModel:\n",
    "        model_state_dict = self.msdd_model.state_dict()\n",
    "        spk_emb_module_names = []\n",
    "        for name in model_state_dict.keys():\n",
    "            if prefix in name:\n",
    "                spk_emb_module_names.append(name)\n",
    "\n",
    "        spk_emb_state_dict = {}\n",
    "        for name in spk_emb_module_names:\n",
    "            org_name = name.replace(prefix, '')\n",
    "            spk_emb_state_dict[org_name] = model_state_dict[name]\n",
    "\n",
    "        _speaker_model = EncDecSpeakerLabelModel.from_config_dict(self.msdd_model.cfg.speaker_model_cfg)\n",
    "        _speaker_model.load_state_dict(spk_emb_state_dict)\n",
    "        return _speaker_model\n",
    "\n",
    "    def _init_msdd_model(self, cfg: Union[DictConfig, NeuralDiarizerInferenceConfig]):\n",
    "        model_path = cfg.diarizer.msdd_model.model_path\n",
    "        if model_path.endswith('.nemo'):\n",
    "            logging.info(f\"Using local nemo file from {model_path}\")\n",
    "            self.msdd_model = EncDecDiarLabelModel.restore_from(restore_path=model_path, map_location=cfg.device)\n",
    "        elif model_path.endswith('.ckpt'):\n",
    "            logging.info(f\"Using local checkpoint from {model_path}\")\n",
    "            self.msdd_model = EncDecDiarLabelModel.load_from_checkpoint(\n",
    "                checkpoint_path=model_path, map_location=cfg.device\n",
    "            )\n",
    "        else:\n",
    "            if model_path not in get_available_model_names(EncDecDiarLabelModel):\n",
    "                logging.warning(f\"requested {model_path} model name not available in pretrained models, instead\")\n",
    "            logging.info(\"Loading pretrained {} model from NGC\".format(model_path))\n",
    "            self.msdd_model = EncDecDiarLabelModel.from_pretrained(model_name=model_path, map_location=cfg.device)\n",
    "        # Load speaker embedding model state_dict which is loaded from the MSDD checkpoint.\n",
    "        if self.use_speaker_model_from_ckpt:\n",
    "            self._speaker_model = self.extract_standalone_speaker_model()\n",
    "        else:\n",
    "            self._speaker_model = None\n",
    "\n",
    "    def get_pred_mat(self, data_list: List[Union[Tuple[int], List[torch.Tensor]]]) -> torch.Tensor:\n",
    "        all_tups = tuple()\n",
    "        for data in data_list:\n",
    "            all_tups += data[0]\n",
    "        n_est_spks = len(set(all_tups))\n",
    "        digit_map = dict(zip(sorted(set(all_tups)), range(n_est_spks)))\n",
    "        total_len = max([sess[1].shape[1] for sess in data_list])\n",
    "        sum_pred = torch.zeros(total_len, n_est_spks)\n",
    "        for _dim_tup, pred_mat in data_list:\n",
    "            dim_tup = [digit_map[x] for x in _dim_tup]\n",
    "            if len(pred_mat.shape) == 3:\n",
    "                pred_mat = pred_mat.squeeze(0)\n",
    "            if n_est_spks <= self.num_spks_per_model:\n",
    "                sum_pred = pred_mat\n",
    "            else:\n",
    "                _end = pred_mat.shape[0]\n",
    "                sum_pred[:_end, dim_tup] += pred_mat.cpu().float()\n",
    "        sum_pred = sum_pred / (n_est_spks - 1)\n",
    "        return sum_pred\n",
    "\n",
    "    def get_integrated_preds_list(\n",
    "        self, uniq_id_list: List[str], test_data_collection: List[Any], preds_list: List[torch.Tensor]\n",
    "    ) -> List[torch.Tensor]:\n",
    "        session_dict = get_id_tup_dict(uniq_id_list, test_data_collection, preds_list)\n",
    "        output_dict = {uniq_id: [] for uniq_id in uniq_id_list}\n",
    "        for uniq_id, data_list in session_dict.items():\n",
    "            sum_pred = self.get_pred_mat(data_list)\n",
    "            output_dict[uniq_id] = sum_pred.unsqueeze(0)\n",
    "        output_list = [output_dict[uniq_id] for uniq_id in uniq_id_list]\n",
    "        return output_list\n",
    "\n",
    "    def get_emb_clus_infer(self, cluster_embeddings):\n",
    "        \"\"\"Assign dictionaries containing the clustering results from the class instance `cluster_embeddings`.\"\"\"\n",
    "        self.msdd_model.emb_sess_test_dict = cluster_embeddings.emb_sess_test_dict\n",
    "        self.msdd_model.clus_test_label_dict = cluster_embeddings.clus_test_label_dict\n",
    "        self.msdd_model.emb_seq_test = cluster_embeddings.emb_seq_test\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def diarize(self) -> Optional[List[Optional[List[Tuple[DiarizationErrorRate, Dict]]]]]:\n",
    "        self.clustering_embedding.prepare_cluster_embs_infer()\n",
    "        self.msdd_model.pairwise_infer = True\n",
    "        self.get_emb_clus_infer(self.clustering_embedding)\n",
    "        preds_list, targets_list, signal_lengths_list = self.run_pairwise_diarization()\n",
    "        thresholds = list(self._cfg.diarizer.msdd_model.parameters.sigmoid_threshold)\n",
    "        return [self.run_overlap_aware_eval(preds_list, threshold) for threshold in thresholds]\n",
    "\n",
    "    def get_range_average(\n",
    "        self, signals: torch.Tensor, emb_vectors: torch.Tensor, diar_window_index: int, test_data_collection: List[Any]\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, int]:\n",
    "        emb_vectors_split = torch.zeros_like(emb_vectors)\n",
    "        uniq_id = os.path.splitext(os.path.basename(test_data_collection.audio_file))[0]\n",
    "        clus_label_tensor = torch.tensor([x[-1] for x in self.msdd_model.clus_test_label_dict[uniq_id]])\n",
    "        for spk_idx in range(len(test_data_collection.target_spks)):\n",
    "            stt, end = (\n",
    "                diar_window_index * self.diar_window_length,\n",
    "                min((diar_window_index + 1) * self.diar_window_length, clus_label_tensor.shape[0]),\n",
    "            )\n",
    "            seq_len = end - stt\n",
    "            if stt < clus_label_tensor.shape[0]:\n",
    "                target_clus_label_tensor = clus_label_tensor[stt:end]\n",
    "                emb_seq, seg_length = (\n",
    "                    signals[stt:end, :, :],\n",
    "                    min(\n",
    "                        self.diar_window_length,\n",
    "                        clus_label_tensor.shape[0] - diar_window_index * self.diar_window_length,\n",
    "                    ),\n",
    "                )\n",
    "                target_clus_label_bool = target_clus_label_tensor == test_data_collection.target_spks[spk_idx]\n",
    "\n",
    "                # There are cases where there is no corresponding speaker in split range, so any(target_clus_label_bool) could be False.\n",
    "                if any(target_clus_label_bool):\n",
    "                    emb_vectors_split[:, :, spk_idx] = torch.mean(emb_seq[target_clus_label_bool], dim=0)\n",
    "\n",
    "                # In case when the loop reaches the end of the sequence\n",
    "                if seq_len < self.diar_window_length:\n",
    "                    emb_seq = torch.cat(\n",
    "                        [\n",
    "                            emb_seq,\n",
    "                            torch.zeros(self.diar_window_length - seq_len, emb_seq.shape[1], emb_seq.shape[2]).to(\n",
    "                                signals.device\n",
    "                            ),\n",
    "                        ],\n",
    "                        dim=0,\n",
    "                    )\n",
    "            else:\n",
    "                emb_seq = torch.zeros(self.diar_window_length, emb_vectors.shape[0], emb_vectors.shape[1]).to(\n",
    "                    signals.device\n",
    "                )\n",
    "                seq_len = 0\n",
    "        return emb_vectors_split, emb_seq, seq_len\n",
    "\n",
    "    def get_range_clus_avg_emb(\n",
    "        self, test_batch: List[torch.Tensor], _test_data_collection: List[Any], device: torch.device('cpu')   # type:ignore\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        _signals, signal_lengths, _targets, _emb_vectors = test_batch\n",
    "        sess_emb_vectors, sess_emb_seq, sess_sig_lengths = [], [], []\n",
    "        split_count = torch.ceil(torch.tensor(_signals.shape[1] / self.diar_window_length)).int()\n",
    "        self.max_pred_length = max(self.max_pred_length, self.diar_window_length * split_count)\n",
    "        for k in range(_signals.shape[0]):\n",
    "            signals, emb_vectors, test_data_collection = _signals[k], _emb_vectors[k], _test_data_collection[k]\n",
    "            for diar_window_index in range(split_count):\n",
    "                emb_vectors_split, emb_seq, seq_len = self.get_range_average(\n",
    "                    signals, emb_vectors, diar_window_index, test_data_collection\n",
    "                )\n",
    "                sess_emb_vectors.append(emb_vectors_split)\n",
    "                sess_emb_seq.append(emb_seq)\n",
    "                sess_sig_lengths.append(seq_len)\n",
    "        sess_emb_vectors = torch.stack(sess_emb_vectors).to(device)\n",
    "        sess_emb_seq = torch.stack(sess_emb_seq).to(device)\n",
    "        sess_sig_lengths = torch.tensor(sess_sig_lengths).to(device)\n",
    "        return sess_emb_vectors, sess_emb_seq, sess_sig_lengths\n",
    "\n",
    "    def diar_infer(\n",
    "        self, test_batch: List[torch.Tensor], test_data_collection: List[Any]\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "       \n",
    "        signals, signal_lengths, _targets, emb_vectors = test_batch\n",
    "        if self._cfg.diarizer.msdd_model.parameters.split_infer:\n",
    "            split_count = torch.ceil(torch.tensor(signals.shape[1] / self.diar_window_length)).int()\n",
    "            sess_emb_vectors, sess_emb_seq, sess_sig_lengths = self.get_range_clus_avg_emb(\n",
    "                test_batch, test_data_collection, device=self.msdd_model.device\n",
    "            )\n",
    "            with autocast():\n",
    "                _preds, scale_weights = self.msdd_model.forward_infer(\n",
    "                    input_signal=sess_emb_seq,\n",
    "                    input_signal_length=sess_sig_lengths,\n",
    "                    emb_vectors=sess_emb_vectors,\n",
    "                    targets=None,\n",
    "                )\n",
    "            _preds = _preds.reshape(len(signal_lengths), split_count * self.diar_window_length, -1)\n",
    "            _preds = _preds[:, : signals.shape[1], :]\n",
    "        else:\n",
    "            with autocast():\n",
    "                _preds, scale_weights = self.msdd_model.forward_infer(\n",
    "                    input_signal=signals, input_signal_length=signal_lengths, emb_vectors=emb_vectors, targets=None\n",
    "                )\n",
    "        self.max_pred_length = max(_preds.shape[1], self.max_pred_length)\n",
    "        preds = torch.zeros(_preds.shape[0], self.max_pred_length, _preds.shape[2])\n",
    "        targets = torch.zeros(_preds.shape[0], self.max_pred_length, _preds.shape[2])\n",
    "        preds[:, : _preds.shape[1], :] = _preds\n",
    "        return preds, targets, signal_lengths\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def run_pairwise_diarization(self) -> Tuple[List[torch.Tensor], List[torch.Tensor], List[torch.Tensor]]:\n",
    "        \n",
    "        self.out_rttm_dir = self.clustering_embedding.out_rttm_dir\n",
    "        self.msdd_model.setup_test_data(self.msdd_model.cfg.test_ds)\n",
    "        self.msdd_model.eval()\n",
    "        cumul_sample_count = [0]\n",
    "        preds_list, targets_list, signal_lengths_list = [], [], []\n",
    "        uniq_id_list = get_uniq_id_list_from_manifest(self.msdd_model.cfg.test_ds.manifest_filepath)\n",
    "        test_data_collection = [d for d in self.msdd_model.data_collection]\n",
    "        for sidx, test_batch in enumerate(tqdm(self.msdd_model.test_dataloader())):\n",
    "            signals, signal_lengths, _targets, emb_vectors = test_batch\n",
    "            cumul_sample_count.append(cumul_sample_count[-1] + signal_lengths.shape[0])\n",
    "            preds, targets, signal_lengths = self.diar_infer(\n",
    "                test_batch, test_data_collection[cumul_sample_count[-2] : cumul_sample_count[-1]]\n",
    "            )\n",
    "            if self._cfg.diarizer.msdd_model.parameters.seq_eval_mode:\n",
    "                self.msdd_model._accuracy_test(preds, targets, signal_lengths)\n",
    "\n",
    "            preds_list.extend(list(torch.split(preds, 1)))\n",
    "            targets_list.extend(list(torch.split(targets, 1)))\n",
    "            signal_lengths_list.extend(list(torch.split(signal_lengths, 1)))\n",
    "\n",
    "        if self._cfg.diarizer.msdd_model.parameters.seq_eval_mode:\n",
    "            f1_score, simple_acc = self.msdd_model.compute_accuracies()\n",
    "            logging.info(f\"Test Inference F1 score. {f1_score:.4f}, simple Acc. {simple_acc:.4f}\")\n",
    "        integrated_preds_list = self.get_integrated_preds_list(uniq_id_list, test_data_collection, preds_list)\n",
    "        return integrated_preds_list, targets_list, signal_lengths_list\n",
    "\n",
    "    def run_overlap_aware_eval(\n",
    "        self, preds_list: List[torch.Tensor], threshold: float\n",
    "    ) -> List[Optional[Tuple[DiarizationErrorRate, Dict]]]:\n",
    "        \n",
    "        logging.info(\n",
    "            f\"     [Threshold: {threshold:.4f}] [use_clus_as_main={self.use_clus_as_main}] [diar_window={self.diar_window_length}]\"\n",
    "        )\n",
    "        outputs = []\n",
    "        manifest_filepath = self.msdd_model.cfg.test_ds.manifest_filepath\n",
    "        rttm_map = audio_rttm_map(manifest_filepath)\n",
    "        for k, (collar, ignore_overlap) in enumerate(self.diar_eval_settings):\n",
    "            all_reference, all_hypothesis = make_rttm_with_overlap(\n",
    "                manifest_filepath,\n",
    "                self.msdd_model.clus_test_label_dict,\n",
    "                preds_list,\n",
    "                threshold=threshold,\n",
    "                infer_overlap=True,\n",
    "                use_clus_as_main=self.use_clus_as_main,\n",
    "                overlap_infer_spk_limit=self.overlap_infer_spk_limit,\n",
    "                use_adaptive_thres=self.use_adaptive_thres,\n",
    "                max_overlap_spks=self.max_overlap_spks,\n",
    "                out_rttm_dir=self.out_rttm_dir,\n",
    "            )\n",
    "            output = score_labels(\n",
    "                rttm_map,\n",
    "                all_reference,\n",
    "                all_hypothesis,\n",
    "                collar=collar,\n",
    "                ignore_overlap=ignore_overlap,\n",
    "                verbose=self._cfg.verbose,\n",
    "            )\n",
    "            outputs.append(output)\n",
    "        logging.info(f\"  \\n\")\n",
    "        return outputs\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(\n",
    "        cls,\n",
    "        model_name: str,\n",
    "        vad_model_name: str = 'vad_multilingual_marblenet',\n",
    "        map_location: Optional[str] = None,\n",
    "        verbose: bool = False,\n",
    "    ):\n",
    "        \n",
    "        logging.setLevel(logging.INFO if verbose else logging.WARNING)\n",
    "        cfg = NeuralDiarizerInferenceConfig.init_config(\n",
    "            diar_model_path=model_name,\n",
    "            vad_model_path=vad_model_name,\n",
    "            map_location=map_location,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "        return cls(cfg)\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        audio_filepath: str,\n",
    "        batch_size: int = 64,\n",
    "        num_workers: int = 1,\n",
    "        max_speakers: Optional[int] = None,\n",
    "        num_speakers: Optional[int] = None,\n",
    "        out_dir: Optional[str] = None,\n",
    "        verbose: bool = False,\n",
    "    ) -> Union[Annotation, List[Annotation]]:\n",
    "        \n",
    "        if out_dir:\n",
    "            os.makedirs(out_dir, exist_ok=True)\n",
    "        with tempfile.TemporaryDirectory(dir=out_dir) as tmpdir:\n",
    "            manifest_path = os.path.join(tmpdir, 'manifest.json')\n",
    "            meta = [\n",
    "                {\n",
    "                    'audio_filepath': audio_filepath,\n",
    "                    'offset': 0,\n",
    "                    'duration': None,\n",
    "                    'label': 'infer',\n",
    "                    'text': '-',\n",
    "                    'num_speakers': num_speakers,\n",
    "                    'rttm_filepath': None,\n",
    "                    'uem_filepath': None,\n",
    "                }\n",
    "            ]\n",
    "\n",
    "            with open(manifest_path, 'w') as f:\n",
    "                f.write('\\n'.join(json.dumps(x) for x in meta))\n",
    "\n",
    "            self._initialize_configs(\n",
    "                manifest_path=manifest_path,\n",
    "                max_speakers=max_speakers,\n",
    "                num_speakers=num_speakers,\n",
    "                tmpdir=tmpdir,\n",
    "                batch_size=batch_size,\n",
    "                num_workers=num_workers,\n",
    "                verbose=verbose,\n",
    "            )\n",
    "\n",
    "            self.msdd_model.cfg.test_ds.manifest_filepath = manifest_path\n",
    "            self.diarize()\n",
    "\n",
    "            pred_labels_clus = rttm_to_labels(f'{tmpdir}/pred_rttms/{Path(audio_filepath).stem}.rttm')\n",
    "        return labels_to_pyannote_object(pred_labels_clus)\n",
    "\n",
    "    def _initialize_configs(\n",
    "        self,\n",
    "        manifest_path: str,\n",
    "        max_speakers: Optional[int],\n",
    "        num_speakers: Optional[int],\n",
    "        tmpdir: tempfile.TemporaryDirectory,\n",
    "        batch_size: int,\n",
    "        num_workers: int,\n",
    "        verbose: bool,\n",
    "    ) -> None:\n",
    "        self._cfg.batch_size = batch_size\n",
    "        self._cfg.num_workers = num_workers\n",
    "        self._cfg.diarizer.manifest_filepath = manifest_path\n",
    "        self._cfg.diarizer.out_dir = tmpdir\n",
    "        self._cfg.verbose = verbose\n",
    "        self._cfg.diarizer.clustering.parameters.oracle_num_speakers = num_speakers is not None\n",
    "        if max_speakers:\n",
    "            self._cfg.diarizer.clustering.parameters.max_num_speakers = max_speakers\n",
    "        self.transfer_diar_params_to_model_params(self.msdd_model, self._cfg)\n",
    "\n",
    "    @classmethod\n",
    "    def list_available_models(cls) -> List[PretrainedModelInfo]:\n",
    "        return EncDecDiarLabelModel.list_available_models()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config=OmegaConf.load('/mnt/d/Programs/Python/PW/projects/speech/diarization/nemo_diarization/config.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'ClusterDiarizer', 'num_workers': 1, 'sample_rate': 16000, 'batch_size': 64, 'device': None, 'verbose': True, 'diarizer': {'manifest_filepath': '/mnt/d/Programs/Python/PW/projects/speech/diarization/nemo_diarization/manifest.txt', 'out_dir': '/mnt/d/Programs/Python/PW/projects/speech/diarization/nemo_diarization/out_dir', 'oracle_vad': False, 'collar': 0.25, 'ignore_overlap': True, 'vad': {'model_path': 'vad_multilingual_marblenet', 'external_vad_manifest': None, 'parameters': {'window_length_in_sec': 0.15, 'shift_length_in_sec': 0.01, 'smoothing': 'median', 'overlap': 0.5, 'onset': 0.1, 'offset': 0.1, 'pad_onset': 0.1, 'pad_offset': 0, 'min_duration_on': 0, 'min_duration_off': 0.2, 'filter_speech_first': True}}, 'speaker_embeddings': {'model_path': 'titanet_large', 'parameters': {'window_length_in_sec': [1.5, 1.25, 1.0, 0.75, 0.5], 'shift_length_in_sec': [0.75, 0.625, 0.5, 0.375, 0.25], 'multiscale_weights': [1, 1, 1, 1, 1], 'save_embeddings': True}}, 'clustering': {'parameters': {'oracle_num_speakers': False, 'max_num_speakers': 8, 'enhanced_count_thres': 80, 'max_rp_threshold': 0.25, 'sparse_search_volume': 30, 'maj_vote_spk_count': False, 'chunk_cluster_count': 50, 'embeddings_per_chunk': 10000}}, 'msdd_model': {'model_path': 'diar_msdd_telephonic', 'parameters': {'use_speaker_model_from_ckpt': True, 'infer_batch_size': 25, 'sigmoid_threshold': [0.7], 'seq_eval_mode': False, 'split_infer': True, 'diar_window_length': 50, 'overlap_infer_spk_limit': 5}}, 'asr': {'model_path': 'stt_en_conformer_ctc_large', 'parameters': {'asr_based_vad': False, 'asr_based_vad_threshold': 1.0, 'asr_batch_size': None, 'decoder_delay_in_sec': None, 'word_ts_anchor_offset': None, 'word_ts_anchor_pos': 'start', 'fix_word_ts_with_VAD': False, 'colored_text': False, 'print_time': True, 'break_lines': False}, 'ctc_decoder_parameters': {'pretrained_language_model': None, 'beam_width': 32, 'alpha': 0.5, 'beta': 2.5}, 'realigning_lm_parameters': {'arpa_language_model': None, 'min_number_of_words': 3, 'max_number_of_words': 10, 'logprob_diff_threshold': 1.2}}}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NemoNeuralDiarizer(LightningModule):\n",
    "    def __init__(self,cfg:OmegaConf):\n",
    "        super(NemoNeuralDiarizer,self).__init__()\n",
    "        self._cfg = cfg\n",
    "\n",
    "        # Parameter settings for MSDD model\n",
    "        self.use_speaker_model_from_ckpt = cfg.diarizer.msdd_model.parameters.get('use_speaker_model_from_ckpt', True)\n",
    "        self.use_clus_as_main = cfg.diarizer.msdd_model.parameters.get('use_clus_as_main', False)\n",
    "        self.max_overlap_spks = cfg.diarizer.msdd_model.parameters.get('max_overlap_spks', 2)\n",
    "        self.num_spks_per_model = cfg.diarizer.msdd_model.parameters.get('num_spks_per_model', 2)\n",
    "        self.use_adaptive_thres = cfg.diarizer.msdd_model.parameters.get('use_adaptive_thres', True)\n",
    "        self.max_pred_length = cfg.diarizer.msdd_model.parameters.get('max_pred_length', 0)\n",
    "        self.diar_eval_settings = cfg.diarizer.msdd_model.parameters.get(\n",
    "            'diar_eval_settings', [(0.25, True), (0.25, False), (0.0, False)]\n",
    "        )\n",
    "\n",
    "        self._init_msdd_model(cfg)\n",
    "        self.diar_window_length = cfg.diarizer.msdd_model.parameters.diar_window_length\n",
    "        self.msdd_model.cfg = self.transfer_diar_params_to_model_params(self.msdd_model, cfg)\n",
    "\n",
    "        # Initialize clustering and embedding preparation instance (as a diarization encoder).\n",
    "        self.clustering_embedding = ClusterEmbedding(\n",
    "            cfg_diar_infer=cfg, cfg_msdd_model=self.msdd_model.cfg, speaker_model=self._speaker_model\n",
    "        )\n",
    "\n",
    "    def extract_standalone_speaker_model(self, prefix: str = 'msdd._speaker_model.') -> EncDecSpeakerLabelModel:\n",
    "        model_state_dict = self.msdd_model.state_dict()\n",
    "        spk_emb_module_names = []\n",
    "        for name in model_state_dict.keys():\n",
    "            if prefix in name:\n",
    "                spk_emb_module_names.append(name)\n",
    "\n",
    "        spk_emb_state_dict = {}\n",
    "        for name in spk_emb_module_names:\n",
    "            org_name = name.replace(prefix, '')\n",
    "            spk_emb_state_dict[org_name] = model_state_dict[name]\n",
    "\n",
    "        _speaker_model = EncDecSpeakerLabelModel.from_config_dict(self.msdd_model.cfg.speaker_model_cfg)\n",
    "        _speaker_model.load_state_dict(spk_emb_state_dict)\n",
    "        return _speaker_model\n",
    "    def transfer_diar_params_to_model_params(self, msdd_model, cfg):\n",
    "        msdd_model.cfg.diarizer.out_dir = cfg.diarizer.out_dir\n",
    "        msdd_model.cfg.test_ds.manifest_filepath = cfg.diarizer.manifest_filepath\n",
    "        msdd_model.cfg.test_ds.emb_dir = cfg.diarizer.out_dir\n",
    "        msdd_model.cfg.test_ds.batch_size = cfg.diarizer.msdd_model.parameters.infer_batch_size\n",
    "        msdd_model.cfg.test_ds.seq_eval_mode = cfg.diarizer.msdd_model.parameters.seq_eval_mode\n",
    "        msdd_model._cfg.max_num_of_spks = cfg.diarizer.clustering.parameters.max_num_speakers\n",
    "        return msdd_model.cfg\n",
    "    \n",
    "    def _init_msdd_model(self, cfg: Union[DictConfig, NeuralDiarizerInferenceConfig]):\n",
    "        model_path = cfg.diarizer.msdd_model.model_path\n",
    "        if model_path.endswith('.nemo'):\n",
    "            logging.info(f\"Using local nemo file from {model_path}\")\n",
    "            self.msdd_model = EncDecDiarLabelModel.restore_from(restore_path=model_path, map_location=cfg.device)\n",
    "        elif model_path.endswith('.ckpt'):\n",
    "            logging.info(f\"Using local checkpoint from {model_path}\")\n",
    "            self.msdd_model = EncDecDiarLabelModel.load_from_checkpoint(\n",
    "                checkpoint_path=model_path, map_location=cfg.device\n",
    "            )\n",
    "        else:\n",
    "            if model_path not in get_available_model_names(EncDecDiarLabelModel):\n",
    "                logging.warning(f\"requested {model_path} model name not available in pretrained models, instead\")\n",
    "            logging.info(\"Loading pretrained {} model from NGC\".format(model_path))\n",
    "            self.msdd_model = EncDecDiarLabelModel.from_pretrained(model_name=model_path, map_location=cfg.device)\n",
    "        # Load speaker embedding model state_dict which is loaded from the MSDD checkpoint.\n",
    "        if self.use_speaker_model_from_ckpt:\n",
    "            self._speaker_model = self.extract_standalone_speaker_model()\n",
    "        else:\n",
    "            self._speaker_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-09-28 00:15:57 nemo_logging:381] Loading pretrained diar_msdd_telephonic model from NGC\n",
      "[NeMo I 2024-09-28 00:15:57 nemo_logging:381] Found existing object /home/rahim/.cache/torch/NeMo/NeMo_2.1.0rc0/diar_msdd_telephonic/3c3697a0a46f945574fa407149975a13/diar_msdd_telephonic.nemo.\n",
      "[NeMo I 2024-09-28 00:15:57 nemo_logging:381] Re-using file from: /home/rahim/.cache/torch/NeMo/NeMo_2.1.0rc0/diar_msdd_telephonic/3c3697a0a46f945574fa407149975a13/diar_msdd_telephonic.nemo\n",
      "[NeMo I 2024-09-28 00:15:57 nemo_logging:381] Instantiating model from pre-trained checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-09-28 00:16:01 nemo_logging:393] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    manifest_filepath: null\n",
      "    emb_dir: null\n",
      "    sample_rate: 16000\n",
      "    num_spks: 2\n",
      "    soft_label_thres: 0.5\n",
      "    labels: null\n",
      "    batch_size: 15\n",
      "    emb_batch_size: 0\n",
      "    shuffle: true\n",
      "    \n",
      "[NeMo W 2024-09-28 00:16:01 nemo_logging:393] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    manifest_filepath: null\n",
      "    emb_dir: null\n",
      "    sample_rate: 16000\n",
      "    num_spks: 2\n",
      "    soft_label_thres: 0.5\n",
      "    labels: null\n",
      "    batch_size: 15\n",
      "    emb_batch_size: 0\n",
      "    shuffle: false\n",
      "    \n",
      "[NeMo W 2024-09-28 00:16:01 nemo_logging:393] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
      "    Test config : \n",
      "    manifest_filepath: null\n",
      "    emb_dir: null\n",
      "    sample_rate: 16000\n",
      "    num_spks: 2\n",
      "    soft_label_thres: 0.5\n",
      "    labels: null\n",
      "    batch_size: 15\n",
      "    emb_batch_size: 0\n",
      "    shuffle: false\n",
      "    seq_eval_mode: false\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-09-28 00:16:01 nemo_logging:381] PADDING: 16\n",
      "[NeMo I 2024-09-28 00:16:02 nemo_logging:381] PADDING: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-09-28 00:16:04 nemo_logging:393] /mnt/d/Programs/Python/PW/projects/speech/speech_env_torch/lib/python3.12/site-packages/nemo/core/connectors/save_restore_connector.py:682: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "      return torch.load(model_weights, map_location='cpu')\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-09-28 00:16:04 nemo_logging:381] Model EncDecDiarLabelModel was successfully restored from /home/rahim/.cache/torch/NeMo/NeMo_2.1.0rc0/diar_msdd_telephonic/3c3697a0a46f945574fa407149975a13/diar_msdd_telephonic.nemo.\n",
      "[NeMo I 2024-09-28 00:16:04 nemo_logging:381] PADDING: 16\n",
      "[NeMo I 2024-09-28 00:16:04 nemo_logging:381] Loading pretrained vad_multilingual_marblenet model from NGC\n",
      "[NeMo I 2024-09-28 00:16:04 nemo_logging:381] Found existing object /home/rahim/.cache/torch/NeMo/NeMo_2.1.0rc0/vad_multilingual_marblenet/670f425c7f186060b7a7268ba6dfacb2/vad_multilingual_marblenet.nemo.\n",
      "[NeMo I 2024-09-28 00:16:04 nemo_logging:381] Re-using file from: /home/rahim/.cache/torch/NeMo/NeMo_2.1.0rc0/vad_multilingual_marblenet/670f425c7f186060b7a7268ba6dfacb2/vad_multilingual_marblenet.nemo\n",
      "[NeMo I 2024-09-28 00:16:04 nemo_logging:381] Instantiating model from pre-trained checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-09-28 00:16:04 nemo_logging:393] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    manifest_filepath: /manifests/ami_train_0.63.json,/manifests/freesound_background_train.json,/manifests/freesound_laughter_train.json,/manifests/fisher_2004_background.json,/manifests/fisher_2004_speech_sampled.json,/manifests/google_train_manifest.json,/manifests/icsi_all_0.63.json,/manifests/musan_freesound_train.json,/manifests/musan_music_train.json,/manifests/musan_soundbible_train.json,/manifests/mandarin_train_sample.json,/manifests/german_train_sample.json,/manifests/spanish_train_sample.json,/manifests/french_train_sample.json,/manifests/russian_train_sample.json\n",
      "    sample_rate: 16000\n",
      "    labels:\n",
      "    - background\n",
      "    - speech\n",
      "    batch_size: 256\n",
      "    shuffle: true\n",
      "    is_tarred: false\n",
      "    tarred_audio_filepaths: null\n",
      "    tarred_shard_strategy: scatter\n",
      "    augmentor:\n",
      "      shift:\n",
      "        prob: 0.5\n",
      "        min_shift_ms: -10.0\n",
      "        max_shift_ms: 10.0\n",
      "      white_noise:\n",
      "        prob: 0.5\n",
      "        min_level: -90\n",
      "        max_level: -46\n",
      "        norm: true\n",
      "      noise:\n",
      "        prob: 0.5\n",
      "        manifest_path: /manifests/noise_0_1_musan_fs.json\n",
      "        min_snr_db: 0\n",
      "        max_snr_db: 30\n",
      "        max_gain_db: 300.0\n",
      "        norm: true\n",
      "      gain:\n",
      "        prob: 0.5\n",
      "        min_gain_dbfs: -10.0\n",
      "        max_gain_dbfs: 10.0\n",
      "        norm: true\n",
      "    num_workers: 16\n",
      "    pin_memory: true\n",
      "    \n",
      "[NeMo W 2024-09-28 00:16:04 nemo_logging:393] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    manifest_filepath: /manifests/ami_dev_0.63.json,/manifests/freesound_background_dev.json,/manifests/freesound_laughter_dev.json,/manifests/ch120_moved_0.63.json,/manifests/fisher_2005_500_speech_sampled.json,/manifests/google_dev_manifest.json,/manifests/musan_music_dev.json,/manifests/mandarin_dev.json,/manifests/german_dev.json,/manifests/spanish_dev.json,/manifests/french_dev.json,/manifests/russian_dev.json\n",
      "    sample_rate: 16000\n",
      "    labels:\n",
      "    - background\n",
      "    - speech\n",
      "    batch_size: 256\n",
      "    shuffle: false\n",
      "    val_loss_idx: 0\n",
      "    num_workers: 16\n",
      "    pin_memory: true\n",
      "    \n",
      "[NeMo W 2024-09-28 00:16:04 nemo_logging:393] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
      "    Test config : \n",
      "    manifest_filepath: null\n",
      "    sample_rate: 16000\n",
      "    labels:\n",
      "    - background\n",
      "    - speech\n",
      "    batch_size: 128\n",
      "    shuffle: false\n",
      "    test_loss_idx: 0\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-09-28 00:16:04 nemo_logging:381] PADDING: 16\n",
      "[NeMo I 2024-09-28 00:16:04 nemo_logging:381] Model EncDecClassificationModel was successfully restored from /home/rahim/.cache/torch/NeMo/NeMo_2.1.0rc0/vad_multilingual_marblenet/670f425c7f186060b7a7268ba6dfacb2/vad_multilingual_marblenet.nemo.\n"
     ]
    }
   ],
   "source": [
    "neural_diarizer=NemoNeuralDiarizer(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
