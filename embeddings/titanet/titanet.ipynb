{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import librosa \n",
    "import os\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prologue(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 kernel_size=3):\n",
    "        super(Prologue,self).__init__()\n",
    "        self.conv=nn.Conv1d(in_channels=in_channels,\n",
    "                            out_channels=out_channels,\n",
    "                            kernel_size=kernel_size,\n",
    "                            padding='same')\n",
    "        self.norm=nn.BatchNorm1d(out_channels)\n",
    "        self.relu=nn.ReLU()\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x=self.conv(x)\n",
    "        x=self.norm(x)\n",
    "        x=self.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 out_channels,\n",
    "                 kernel_size,\n",
    "                 dilation=1):\n",
    "        super(SubBlock,self).__init__()\n",
    "        self.depthwise_conv=nn.Conv1d(in_channels=out_channels,\n",
    "                                      out_channels=out_channels,\n",
    "                                      kernel_size=kernel_size,\n",
    "                                      padding=kernel_size//2,\n",
    "                                      dilation=dilation)\n",
    "        self.pointwise_conv=nn.Conv1d(in_channels=out_channels,\n",
    "                                      out_channels=out_channels,\n",
    "                                      kernel_size=1)\n",
    "        self.norm=nn.BatchNorm1d(out_channels)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.dropout=nn.Dropout()\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.depthwise_conv(x)\n",
    "        x=self.pointwise_conv(x)\n",
    "        x=self.norm(x)\n",
    "        x=self.relu(x)\n",
    "        x=self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SqueezeExcitation(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 reduction):\n",
    "        super(SqueezeExcitation,self).__init__()\n",
    "        self.squeeze=nn.AdaptiveAvgPool1d(1)\n",
    "        self.linear1=nn.Linear(in_channels,in_channels//reduction)\n",
    "        self.linear2=nn.Linear(in_channels//reduction,in_channels)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.gate=nn.Sigmoid()\n",
    "    \n",
    "    def forward(self,x):\n",
    "        input=x\n",
    "        x=self.squeeze(x)\n",
    "        x=x.squeeze(-1)\n",
    "        x=self.linear1(x)\n",
    "        x=self.relu(x)\n",
    "        x=self.linear2(x)\n",
    "        x=self.gate(x)\n",
    "        x=x.unsqueeze(-1)\n",
    "        return input*x.expand_as(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MegaBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 out_channels,\n",
    "                 kernel_size,\n",
    "                 dilation,\n",
    "                 repeat,\n",
    "                 reduction):\n",
    "        super(MegaBlock,self).__init__()\n",
    "        self.sub_block_list=[\n",
    "            SubBlock(\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=kernel_size,\n",
    "                dilation=dilation\n",
    "            ) for _ in range(repeat)\n",
    "        ]\n",
    "        self.repeat_block=nn.Sequential(*self.sub_block_list)\n",
    "        self.depthwise_conv1=nn.Conv1d(in_channels=out_channels,\n",
    "                                       out_channels=out_channels,\n",
    "                                       kernel_size=kernel_size,\n",
    "                                       padding=kernel_size//2)\n",
    "        self.depthwise_conv2=nn.Conv1d(in_channels=out_channels,\n",
    "                                       out_channels=out_channels,\n",
    "                                       kernel_size=kernel_size,\n",
    "                                       padding=kernel_size//2)\n",
    "        self.dropout=nn.Dropout()\n",
    "        self.relu=nn.ReLU()\n",
    "        self.pointwise_conv=nn.Conv1d(in_channels=out_channels,\n",
    "                                      out_channels=out_channels,\n",
    "                                      kernel_size=1)\n",
    "        self.norm=nn.BatchNorm1d(out_channels)\n",
    "        self.se_block=SqueezeExcitation(out_channels,reduction)\n",
    "\n",
    "    def forward(self,x):\n",
    "        y=self.repeat_block(x)\n",
    "        y=self.depthwise_conv1(y)\n",
    "        y=self.depthwise_conv2(y)\n",
    "        y=self.se_block(y)\n",
    "        x=self.pointwise_conv(x)\n",
    "        x=self.norm(x)\n",
    "        result=self.relu(x+y)\n",
    "        result=self.dropout(result)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Epilogue(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 kernel_size=1):\n",
    "        super(Epilogue,self).__init__()\n",
    "        self.conv=nn.Conv1d(in_channels=in_channels,\n",
    "                            out_channels=out_channels,\n",
    "                            kernel_size=kernel_size)\n",
    "        self.norm=nn.BatchNorm1d(out_channels)\n",
    "        self.relu=nn.ReLU()\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x=self.conv(x)\n",
    "        x=self.norm(x)\n",
    "        x=self.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 prolog_in_channels=80,\n",
    "                 prolog_out_channels=256,\n",
    "                 epilog_out_channels=256,\n",
    "                 kernel_b1=7,\n",
    "                 dilation_b1=1,\n",
    "                 repeat_b1=2,\n",
    "                 reduction_b1=16,\n",
    "                 kernel_b2=11,\n",
    "                 dilation_b2=1,\n",
    "                 repeat_b2=2,\n",
    "                 reduction_b2=16,\n",
    "                 kernel_b3=15,\n",
    "                 dilation_b3=1,\n",
    "                 repeat_b3=2,\n",
    "                 reduction_b3=16):\n",
    "        super(Encoder,self).__init__()\n",
    "        self.prolog=Prologue(in_channels=prolog_in_channels,\n",
    "                             out_channels=prolog_out_channels)\n",
    "        self.block1=MegaBlock(out_channels=prolog_out_channels,kernel_size=kernel_b1,dilation=dilation_b1,repeat=repeat_b1,reduction=reduction_b1)\n",
    "        self.block2=MegaBlock(out_channels=prolog_out_channels,kernel_size=kernel_b2,dilation=dilation_b2,repeat=repeat_b2,reduction=reduction_b2)\n",
    "        self.block3=MegaBlock(out_channels=prolog_out_channels,kernel_size=kernel_b3,dilation=dilation_b3,repeat=repeat_b3,reduction=reduction_b3)\n",
    "        self.epilog=Epilogue(in_channels=prolog_out_channels,out_channels=epilog_out_channels)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x=self.prolog(x)\n",
    "        x=self.block1(x)\n",
    "        x=self.block2(x)\n",
    "        x=self.block3(x)\n",
    "        x=self.epilog(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.rand(size=[2,80,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentiveStatisticalPooling(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_size,\n",
    "                 hidden_size,\n",
    "                 eps=1e-8):\n",
    "        super(AttentiveStatisticalPooling,self).__init__()\n",
    "        self.eps=eps\n",
    "        self.linear1=nn.Linear(in_size,hidden_size)\n",
    "        self.linear2=nn.Linear(hidden_size,in_size)\n",
    "        self.tanh=nn.Tanh()\n",
    "        self.softmax=nn.Softmax(dim=2)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        input=x\n",
    "        x=self.linear1(x.transpose(1,2))\n",
    "        x=self.tanh(x)\n",
    "        e_t=self.linear2(x)\n",
    "        alpha_t=self.softmax(e_t.transpose(1,2))\n",
    "        means=torch.sum(alpha_t*input,dim=2)\n",
    "        residuals=torch.sum(alpha_t * input**2,dim=2)-means**2\n",
    "        stds=torch.sqrt(residuals.clamp(min=self.eps))\n",
    "        return torch.cat([means,stds],dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_size,\n",
    "                 hidden_size,\n",
    "                 num_class,\n",
    "                 eps=1e-8):\n",
    "        super(Decoder,self).__init__()\n",
    "        self.attention=AttentiveStatisticalPooling(in_size=in_size,\n",
    "                                                  hidden_size=hidden_size,\n",
    "                                                  eps=eps)\n",
    "        self.norm1=nn.BatchNorm1d(in_size*2)\n",
    "        self.linear1=nn.Linear(in_features=in_size*2,out_features=192)\n",
    "        self.norm2=nn.BatchNorm1d(192)\n",
    "        self.linear2=nn.Linear(in_features=192,out_features=num_class)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x=self.attention(x)\n",
    "        x=self.norm1(x)\n",
    "        x=self.linear1(x)\n",
    "        embeddings=self.norm2(x)\n",
    "        logits=self.linear2(embeddings)\n",
    "        return logits,embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TiTaNet(nn.Module):\n",
    "    def __init__(self,\n",
    "                 prolog_in_channels=80,\n",
    "                 prolog_out_channels=256,\n",
    "                 epilog_out_channels=1536,\n",
    "                 kernel_b1=3,\n",
    "                 dilation_b1=1,\n",
    "                 repeat_b1=3,\n",
    "                 reduction_b1=16,\n",
    "                 kernel_b2=3,\n",
    "                 dilation_b2=1,\n",
    "                 repeat_b2=3,\n",
    "                 reduction_b2=16,\n",
    "                 kernel_b3=3,\n",
    "                 dilation_b3=1,\n",
    "                 repeat_b3=3,\n",
    "                 reduction_b3=16,\n",
    "                 hidden_size=128,\n",
    "                 num_class=100,\n",
    "                 eps=1e-8):\n",
    "        super(TiTaNet,self).__init__()\n",
    "        self.encoder=Encoder(\n",
    "                prolog_in_channels,\n",
    "                 prolog_out_channels,\n",
    "                 epilog_out_channels,\n",
    "                 kernel_b1,\n",
    "                 dilation_b1,\n",
    "                 repeat_b1,\n",
    "                 reduction_b1,\n",
    "                 kernel_b2,\n",
    "                 dilation_b2,\n",
    "                 repeat_b2,\n",
    "                 reduction_b2,\n",
    "                 kernel_b3,\n",
    "                 dilation_b3,\n",
    "                 repeat_b3,\n",
    "                 reduction_b3)\n",
    "        self.decoder=Decoder(\n",
    "                 epilog_out_channels,\n",
    "                 hidden_size,\n",
    "                 num_class,\n",
    "                 eps)\n",
    "    def forward(self,x):\n",
    "        x=self.encoder(x)\n",
    "        return self.decoder(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 103]), torch.Size([2, 192]))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanet=TiTaNet(num_class=103)\n",
    "y=titanet(x)\n",
    "y[0].shape,y[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.245335"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in titanet.parameters() if p.requires_grad)/1_000_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdditiveAngularMarginLoss(nn.Module):\n",
    "    def __init__(self, scale=20.0, margin=1.35):\n",
    "        super().__init__()\n",
    "\n",
    "        self.eps = 1e-7\n",
    "        self.scale = scale\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, logits, labels):\n",
    "        numerator = self.scale * torch.cos(\n",
    "            torch.acos(torch.clamp(torch.diagonal(logits.transpose(0, 1)[labels]), -1.0 + self.eps, 1 - self.eps))\n",
    "            + self.margin\n",
    "        )\n",
    "        excl = torch.cat(\n",
    "            [torch.cat((logits[i, :y], logits[i, y + 1 :])).unsqueeze(0) for i, y in enumerate(labels)], dim=0\n",
    "        )\n",
    "        denominator = torch.exp(numerator) + torch.sum(torch.exp(self.scale * excl), dim=1)\n",
    "        L = numerator - torch.log(denominator)\n",
    "        return -torch.mean(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TiTaNetDataset(Dataset):\n",
    "    def __init__(self,data_folder,\n",
    "                 sample_rate=16_000,\n",
    "                 audio_len=1,\n",
    "                 file_ext='wav'):\n",
    "        super(TiTaNetDataset,self).__init__()\n",
    "        self.data_folder=data_folder\n",
    "        self.audio_len=audio_len\n",
    "        self.sample_rate=sample_rate\n",
    "        self.file_ext=file_ext\n",
    "        self.audio_files=glob(f\"{self.data_folder}/**/*.{file_ext}\")\n",
    "        self.labels=os.listdir(self.data_folder)\n",
    "        self.arg_to_label={k:v for v,k in enumerate(self.labels)}\n",
    "        self.label_to_arg={k:v for k,v in enumerate(self.labels)}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.audio_files)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        file=self.audio_files[index]\n",
    "        data,_=librosa.load(file,sr=self.sample_rate,mono=True)\n",
    "        label=file.split('/')[-2]\n",
    "        label=self.arg_to_label[label]\n",
    "        if data.shape[0]>=self.audio_len*self.sample_rate:\n",
    "            data=data[:self.audio_len*self.sample_rate]\n",
    "        if data.shape[0]<self.audio_len*self.sample_rate:\n",
    "            data=np.pad(data, (0,self.audio_len*self.sample_rate - data.shape[0]), mode='constant')\n",
    "        mels=librosa.feature.melspectrogram(y=data,sr=self.sample_rate,n_fft=512,hop_length=160,win_length=400,n_mels=80)\n",
    "        mels=librosa.power_to_db(mels,ref=np.max)\n",
    "        return torch.tensor(mels),torch.tensor(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir=\"/mnt/c/Users/rahim/Downloads/archive/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4017"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files=glob(f\"{data_dir}/**/*.flac\")\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=TiTaNetDataset(data_folder=data_dir,file_ext='flac')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0001': 0,\n",
       " '0002': 1,\n",
       " '0003': 2,\n",
       " '0004': 3,\n",
       " '0005': 4,\n",
       " '0006': 5,\n",
       " '0007': 6,\n",
       " '0008': 7,\n",
       " '0009': 8,\n",
       " '0010': 9,\n",
       " '0011': 10,\n",
       " '0012': 11,\n",
       " '0013': 12,\n",
       " '0014': 13,\n",
       " '0015': 14,\n",
       " '0016': 15,\n",
       " '0017': 16,\n",
       " '0018': 17,\n",
       " '0019': 18,\n",
       " '0020': 19,\n",
       " '0021': 20,\n",
       " '0022': 21,\n",
       " '0023': 22,\n",
       " '0024': 23,\n",
       " '0025': 24,\n",
       " '0026': 25,\n",
       " '0027': 26,\n",
       " '0028': 27,\n",
       " '0029': 28,\n",
       " '0030': 29,\n",
       " '0031': 30,\n",
       " '0032': 31,\n",
       " '0033': 32,\n",
       " '0034': 33,\n",
       " '0035': 34,\n",
       " '0036': 35,\n",
       " '0037': 36,\n",
       " '0038': 37,\n",
       " '0039': 38,\n",
       " '0040': 39,\n",
       " '0041': 40,\n",
       " '0042': 41,\n",
       " '0043': 42,\n",
       " '0044': 43,\n",
       " '0045': 44,\n",
       " '0046': 45,\n",
       " '0047': 46,\n",
       " '0048': 47,\n",
       " '0049': 48,\n",
       " '0050': 49,\n",
       " '0051': 50,\n",
       " '0052': 51,\n",
       " '0053': 52,\n",
       " '0054': 53,\n",
       " '0055': 54,\n",
       " '0056': 55,\n",
       " '0057': 56,\n",
       " '0058': 57,\n",
       " '0059': 58,\n",
       " '0060': 59,\n",
       " '0061': 60,\n",
       " '0062': 61,\n",
       " '0063': 62,\n",
       " '0064': 63,\n",
       " '0065': 64,\n",
       " '0066': 65,\n",
       " '0067': 66,\n",
       " '0068': 67,\n",
       " '0069': 68,\n",
       " '0070': 69,\n",
       " '0071': 70,\n",
       " '0072': 71,\n",
       " '0073': 72,\n",
       " '0074': 73,\n",
       " '0075': 74,\n",
       " '0076': 75,\n",
       " '0077': 76,\n",
       " '0078': 77,\n",
       " '0079': 78,\n",
       " '0080': 79,\n",
       " '0081': 80,\n",
       " '0082': 81,\n",
       " '0083': 82,\n",
       " '0084': 83,\n",
       " '0085': 84,\n",
       " '0086': 85,\n",
       " '0087': 86,\n",
       " '0088': 87,\n",
       " '0089': 88,\n",
       " '0090': 89,\n",
       " '0091': 90,\n",
       " '0092': 91,\n",
       " '0093': 92,\n",
       " '0094': 93,\n",
       " '0095': 94,\n",
       " '0096': 95,\n",
       " '0097': 96,\n",
       " '0098': 97,\n",
       " '0099': 98,\n",
       " '0100': 99,\n",
       " '0101': 100,\n",
       " '0102': 101,\n",
       " '0103': 102}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.arg_to_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=torch.optim.Adam(titanet.parameters())\n",
    "criterion=AdditiveAngularMarginLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanet=titanet.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader=DataLoader(dataset=dataset,batch_size=10,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "660d7edf0f754b9da45479cbf1e9ae35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/402 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18032.066257476807\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1):\n",
    "    titanet.train()\n",
    "    train_loss=0\n",
    "    for x,y in tqdm(dataloader):\n",
    "        x=x.to(device)\n",
    "        y=y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred=titanet(x)\n",
    "        loss=criterion(pred[0],y)\n",
    "        train_loss+=loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
